math_modeling_criteria_generator:
  description: |
    数学建模任务评价
  system: |-
    你是一名全国研究生数学建模竞赛的资深命题与阅卷专家。回答时始终从“题目要考什么——按题型拆解关键能力”的角度出发，强调数学建模思维（先建模再选算法、再验证），并以便于阅卷/制订评分细则的结构化形式输出。
  zh:
    number_of_questions: |-
      {{question}}
      请仔细阅读上述数学建模任务描述，识别其中包含的所有**可独立执行的子任务**。
      每个子任务应是一个逻辑上独立、可执行的分析或建模步骤。
      若某个子任务需要依赖前面任务的结果（例如：数据预处理依赖原始数据采集），则在"depend_on_prev_tasks"中明确列出其依赖的子任务编号。
      只有在确实存在先后依赖时，才填写该字段。

      ✅ 输出格式：
      ```json
      {
        "number_of_tasks": [子任务总数],
        "subtask_1": {
          "description": "子任务1的完整描述",
          "depend_on_prev_tasks": []
        },
        "subtask_2": {
          "description": "子任务2的完整描述",
          "depend_on_prev_tasks": []
        },
        "subtask_3": {
          "description": "子任务3的完整描述",
          "depend_on_prev_tasks": ["subtask_1", "subtask_2"]
        }
      }
      ```
      ⚙️ 生成规则：
      1. 优先根据任务目标、输入输出关系来划分子任务；
      2. 不强制所有子任务都填写依赖，仅在存在显式逻辑关联时才填写；
      3. 子任务描述应完整复述原题中该部分的任务目标与要求，而非仅关键词；
      4. 若题中有综合性任务（如“综合前面模型给出优化建议”），需在依赖字段中明确依赖关系。

    eval_dimension: |-
      📜 背景信息：
      {{background}}

      📜 问题描述：
      {{question}}

      你现在的任务是：针对一个已明确的数学建模子任务，生成**基于任务语义理解的细粒度评分细则**。

      {% if previous_subtasks %}
      📍 依赖子任务：{{previous_subtasks}}
      {% endif %}
      🧩 当前子任务：
      {{subtask}}

      —— 任务说明（请严格遵守） ——
      你的任务：针对上面已明确的**单一数学建模子任务**，生成**基于任务语义理解的细粒度评分细则（JSON）**，要求如下：

      一、先进行**任务语义理解与抽象**（输出项位于 `task_understanding`）：
        1. `core_goal`：一句话概括该子任务的**核心建模目标**，必须包含子任务中的关键动作词（如“预测”“优化”“拟合”“评估”）和对象（如“AQI”“功率曲线”“成本”）。
        2. `expected_output`：明确列出预期输出的形式（数值序列、最优解、图表、报表、模型文件、代码等）。
        3. `key_inputs_constraints`：列出**具体输入变量名 / 数据类型 / 必要前置条件 / 关键约束**（至少 3 项，若不足尽量写“无”并说明原因）。
        4. `modeling_type`：从{预测/优化/仿真/拟合/分类/评价/混合}中选择最贴切的一项（可复合，如“预测+优化”）。
        5. `role_in_pipeline`：说明该子任务在整体建模流程中的定位（例如“作为特征工程后、模型训练前的子任务，依赖前序数据清洗”或“作为最终验证环节，不依赖代码实现”等），并列出直接依赖的前序/后序任务。

      二、基于上述 `task_understanding`，为以下**7 个维度**生成细化评分细则（输出项位于 `evaluation_criteria`）。
        - 维度列表（不可更改顺序）：问题识别、问题复述、假设建立、模型构建、模型求解、代码实现、结果分析。
        - 每个维度必须包含**3–5 个具体 `sub_criteria`**，每个 `sub_criteria` 包含字段：
            - `sub_criteria`：评分点主题（必须包含与 `task_understanding` 中提取的任务关键词之一或多项）
            - `description`：与该**子任务语义**深度绑定的评分说明（不得使用模板化“能否理解任务目标”此类空泛语句）
            - `score`：该评分点在该维度中的分值（整数）。**每个维度内所有 sub_criteria 的分值之和必须等于 100**（便于程序化合成维度得分）
            - `evaluation_focus`：考察能力的简短标签（例如“建模思维/数理推导/算法选择/代码可靠性/结果解释”等；长度 ≤ 6 词）
            - `scoring_hint`：区分**高分 / 中分 / 低分**的具体、可判定的表现要点（每一档至少 1–2 条，最好给出可量化或可检验的迹象）
        - 评分点撰写规则：
            1. **必须**直接从 `task_understanding` 的具体文本中抽取或复写任务关键词（例如若 `core_goal` 有“24 小时 AQI 预测”，则某个 `sub_criteria` 的 `sub_criteria` 或 `description` 必须出现“24 小时”“AQI”“预测”等词）。
            2. 禁止使用泛化或空泛表达（如“目标是否明确”）；必须改写为任务特定的评价（如“是否明确区分小时级 AQI 预测与日均 AQI 预测并给出对应误差指标”）。
            3. `scoring_hint` 要给出可操作的判定标准（例如“高分：小时级 RMSE ≤ X 或 MAE ≤ Y；中分：RMSE 在 X–2X；低分：未提供误差指标或误差显著高于基准”）。若无法量化则给出明确的质量判断语句。
            4. 若某维度**不适用**（例如该子任务无需代码实现），该维度应返回空数组并在该维度下增加一条 `not_applicable_reason` 说明原因。

      三、输出格式与其它要求：
        - 必须只返回一个完全符合下述 JSON 结构的对象（不应包含任何多余文字或注释），保证 JSON 可被直接 parse。
        - JSON 编码须为 UTF-8，字段顺序须与示例保持一致（`task_understanding` 在前，`evaluation_criteria` 在后）。
        - 所有 `description` 和 `scoring_hint` 字段不得超过 300 字。
        - 若提示词中的某处缺失必要信息（例如 `{subtask}` 非明确句子），请**基于可用信息作最合理的假设并写入 `task_understanding.assumptions` 字段（此字段可选）**，不要再次向用户提问。
        - 最后只输出 JSON（不允许输出其他解释性文字）。

      ✅ 输出格式如下：
      ```json
      {
        "task_understanding": {
          "core_goal": "（结合子任务语义自动生成的核心目标）",
          "expected_output": "（模型或结果的预期形式）",
          "key_inputs_constraints": "（主要输入条件与约束说明）",
          "modeling_type": "（如预测/优化/仿真/拟合/分类/评价等）",
          "role_in_pipeline": "（该任务在整个建模流程中的功能定位与依赖关系）"
        },
        "evaluation_criteria": {
          "问题识别": [],
          "问题复述": [],
          "假设建立": [],
          "模型构建": [],
          "模型求解": [],
          "代码实现": [],
          "结果分析": []
        }
      }
      ```

  en:
    number_of_questions: |-
      {{question}}
      Please identify all subproblems contained in the mathematical modeling task described above. List the total number of subproblems and provide a full description for each one.
      ✅ Output Format:
      ```json
      {
        "number_of_questions": [Total number of subproblems],
        "SubProblem_1": "[Full description of Subproblem 1]",
        "SubProblem_2": "[Full description of Subproblem 2]",
        ...
      }
      ```

    eval_dimension: |-
      ✍️ Mathematical Modeling Task:
      {{question}}

      🧩 Current Subproblem:
      {{subproblem}}

      You are developing the evaluation criteria for the **"{{eval_dimension}}"** aspect of Subproblem {{number_of_subproblem}} in the modeling task above (you may refer to but are not limited to the following aspects).
      First, clarify the content of this subproblem. Then, design a reasonable set of scoring dimensions and descriptions for a **10-point scale** based on the "{{eval_dimension}}" aspect.

      🌟 Reference dimensions for "{{eval_dimension}}":
      {{eval_dimension_ref}}

      ✅ Output Format (JSON):

      ```json
      {
        "problem": "[Description of Subproblem {{number_of_subproblem}}]",
        "evaluation_criteria": [
          {
            "dimension": "[Name of evaluation dimension]",
            "description": "[Description of this dimension]",
            "score": [Score assigned to this dimension]
          },
          {
            "dimension": "[Name of evaluation dimension]",
            "description": "[Description of this dimension]",
            "score": [Score assigned to this dimension]
          }
          ...
        ]
      }
      ```


model_key_insight:
  system: |-
    你是一名全国研究生数学建模竞赛的资深命题与阅卷专家。回答时始终从“题目要考什么——按题型拆解关键能力”的角度出发，强调数学建模思维（先建模再选算法、再验证），并以便于阅卷/制订评分细则的结构化形式输出。
  task: |-
    我将为你提供一份【数学建模题目】和一篇【优秀论文】，请完成两步：
    1) 精准识别该题目最接近的题型（从题型库中选择：最优化/运筹（Optimization）、时间序列与预测（Time Series）、网络/图论（Graph & Network）、排队/排程（Queueing & Scheduling）、动力学/连续系统（Dynamical Systems & PDE/ODE）、概率/随机过程（Stochastic Models）、组合与搜索（Combinatorial）、数据拟合与回归（Fitting/Regression）、图像/信号处理（Image/Signal）、实验设计/统计推断（Design & Inference）、混合多阶段决策/Agent模型（Multi-stage/Decision）、其他—请说明）并给出判定理由（用一句话说明为何属于该类型，结合题目关键词）。

    2) 针对该题型，从命题者与阅卷专家视角生成**本题的核心考点清单**（细化到“考点级别”——能直接用于出题说明或评分要点）。要求输出为**JSON 列表**，每个元素为一个“核心考点段落”，字段如下：
    - id: 整数 id
    - theme: 要点主题（简短）
    - purpose: 该要点考察的能力与目标（1-2句）
    - key_points: 该要点下的细化检查点数组（每项短句，尽量可量化）
    - suggested_score_weight: 建议分值占比（百分比形式，整数，所有要点之和应为100，可由模型合理分配）
    - how_to_expand: 如何把此要点扩展为评分细则（给审卷员的判分提示，至少3条）

    额外要求：
    - 列表项尽量细化（每项 key_points 至少 3 条），覆盖题型内常见陷阱与优秀解法应体现的“深度”与“创新”。
    - 若题目为“复合题（多个题型混合）”，先列出主/次题型并分别产出对应的子考点（用同样 JSON 结构，主次在 identified_type 中注明，例如 "Optimization (主) + Time Series (次)"）。
    - **核心目标是提炼题目的考察要点**——即命题人真正希望学生在建模、推理与验证中展现的能力。
    - **优秀论文仅作为参考材料，用于辅助判断考点是否被覆盖或体现；不得照搬论文结构或结论。**
    - 输出语言务必简洁，避免冗词；必须是**有效的 JSON**（便于后续自动化处理）。
    - 输出中应优先标注那些“直接影响模型正确性 / 可解释性 / 可复现性”的考点（例如关键变量定义、一致性假设、边界条件、收敛性/复杂度说明、鲁棒性/灵敏度分析、实验设置/基线比较）。
    - 以```json ```格式输出

    题型示例（给模型的内部参考；生成时无需输出这些示例，但请依此为依据细化考点）：
    - 最优化/运筹（Optimization）关键考点示例：
      - 是否明确目标函数与约束（凸性/可行域）；
      - 变量与尺度归一化/单位一致性；
      - 求解方法选择理由（解析/数值/启发式），算法复杂度与收敛性证明或实验说明；
      - 可行性、算例规模扩展能力、时间复杂度与实现细节（伪代码/工程化说明）；
      - 敏感性/参数分析、模型松弛或近似误差界；
      - 与基线或启发式方法定量比较、统计显著性检验。
    - 时间序列与预测关键考点示例：
      - 数据预处理与缺失值处理策略、稳态/季节性判别；
      - 模型选择与假设（ARIMA/LSTM/Prophet等）及超参调优策略；
      - 交叉验证/滚动预测的评估设计、误差指标与置信区间；
      - 异常检测/模型鲁棒性、多步预测误差传播分析。
    （……可参考更多题型库）

    🏆【优秀论文】
    {{excellent_report}}
    📋 【题目】
     {{question}}

    ✅ 【输出示例】：
    ```json
    {
      "identified_type":"Optimization",
      "reason":"题目要求在约束下最小化运输成本，明显为组合+连续最优化问题",
      "core_checkpoints":[
        {
          "id":1,
          "theme":"目标函数与约束的数学化",
          "purpose":"检验选手是否能把实际问题准确转化为数学目标与约束",
          "key_points":["清晰定义目标函数（含单位）","列出所有约束并解释物理/业务含义","判断目标/约束的凸性或离散性"],
          "suggested_score_weight":20,
          "how_to_expand":["若目标函数含隐含项，扣分并要求改正","若未说明单位或量纲，扣一半分","若能证明凸性或给出近似界，给加分"]
        },
        ...
      ]
    }
    ```

math_modeling_report_eval:
  system: |-
    你是一名**全国研究生数学建模竞赛的资深命题与评审专家**。
    你的任务是基于官方评分标准，对选手论文中某个子问题的建模质量进行**客观、细致、可追溯的评估**。

    ### 🎯 核心评审理念
    你应始终从“**题目考察意图——关键能力拆解——建模逻辑链条**”出发，进行逐项分析与评分。

    ### 🧠 评审原则
    1. **评审视角**
       - 以命题专家（考察核心能力）和阅卷专家（执行评分标准）双重身份进行判断。
       - 你的评语必须明确指出：论文是否真正体现了题目所要求的核心建模能力。

    2. **建模导向**
       - 强调完整的数学建模逻辑：
         🪞 问题理解 → 模型假设与变量定义 → 数学模型建立 → 算法设计与求解 → 模型检验与结果分析 → 优化与拓展。
       - 在评语中体现逻辑链条，不得仅以文字描述或结果判断代替推理过程。

    3. **评分依据**
       - 严格依据输入的【评分准则】，逐项评分。
       - 每个评分项都应明确：
         - 是否满足评分项描述；
         - 满足或不满足的理由；
         - 在给定分值范围内的合理得分。

    4. **结构化输出**
       - 所有回答必须采用结构化 JSON 输出（见下方格式要求）。
       - 各评分项须包含：
         - `"dimension"`：评分维度名称（与评分准则一致）
         - `"comment"`：基于建模逻辑链条的简明评语
         - `"score"`：得分（在指定范围内）

    5. **表达要求**
       - 使用专业、客观、审稿式语言；
       - 禁止对论文内容进行修改或补写；
       - 不输出额外解释、总结或结论。
  zh: |-
    🧩 当前子问题：
    {{subproblem}}

    📋 数学建模论文的内容：
    {{report_content}}

    📜 评分准则：
    ```json
    {{report_criteria}}
    ```
    请你充当一位严格且专业的技术评审专家，依据我提供的【评分准则】和【问题背景】，对一段【数学建模论文的内容】进行逐项评分与评语分析。
    请遵循以下要求完成评分：
    1. 请依次提供每个评分项的分值和评语说明;
    2. 严格依据评分准则中的每一个维度进行评分，在每个评分项中明确指出是否满足评分项描述，并说明原因；
    3. 每个评分项请给出准确分值（在该评分项的指定分值范围内），并附简洁清晰的评语说明；
    4. 保持专业性与客观性，重点关注技术表达的完整性、准确性、建模逻辑与公式推导清晰性；
    5. 不要对内容做修改，也不要生成新的答案，仅评估已有回复的质量。

    ✅ 输出格式（JSON）：
    ```json
    {
      "问题识别": [
        {
          "dimension": "[评分维度名称]",
          "comment": "[评语说明]",
          "score": [得分]
        },
        ...
      ],
      "问题复述": [
        {
          "dimension": "[评分维度名称]",
          "comment": "[评语说明]",
          "score": [得分]
        },
        ...
      ]
    }
    ```
  en: |-
    🧩 Current subproblem:
    {{subproblem}}

    📋 Mathematical modeling report content:
    {{report_content}}

    📜 Evaluation criteria:
    ```json
    {{report_criteria}}
    ```
    You are a strict and professional technical reviewer, according to the provided evaluation criteria and background problem, analyze and score a piece of mathematical modeling report content.
    Please follow the following requirements to complete the scoring:
    1. Provide a total score first, and then provide the score and comment for each scoring item;
    2. Strictly follow the evaluation criteria for each dimension and provide a clear explanation of whether the scoring item is satisfied and explain the reason;
    3. Provide accurate score (within the specified score range) for each scoring item and provide a concise and clear comment;
    4. Maintain professionalism and objectivity, focus on the completeness, accuracy, and clarity of technical expression and mathematical derivation;
    5. Do not modify the content, do not generate new answers, and only evaluate the quality of the existing replies.

    ✅ Output format (JSON):
    ```json
    {
      "total_score": [Total score],
      "evaluation_criteria": [
        {
          "dimension": "[Evaluation dimension name]",
          "score": [Score],
          "comment": "[Comment]"
        },
        {
          "dimension": "[Evaluation dimension name]",
          "score": [Score],
          "comment": "[Comment]"
        },
        ...
      ]
    }
    ```