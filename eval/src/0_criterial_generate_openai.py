"""
This script generates the core criteria of a problem.

Usage:
    python eval/src/0_criterial_generate_openai.py \
        --problem-dir MMBench/CPMCM/problem \
        --criterial-prompt eval/prompts/criterial_generate.yaml \
        --config-path config.yaml \
        --output-dir MMBench/CPMCM/criteria
        --tmp-dir tmp \
        --model-name gpt-5-mini
"""

import json
import argparse
import logging
from pathlib import Path
import traceback
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    RetryCallState,
)
from dotenv import load_dotenv

from utils import (
    write_json,
    populate_template,
    load_json,
    clean_json_txt,
    find_task_id_from_path,
    load_yaml,
)

from agent.data_description import DataDescription
from agent.problem_analysis import ProblemUnderstanding
from agent.coordinator import Coordinator
from agent.problem_decompse import ProblemDecompose
from llm.llm import LLM
from prompt.template import PROBLEM_PROMPT

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv(override=True)


def get_problem(problem_path: Path, llm: LLM) -> tuple[str, dict]:
    problem = load_json(problem_path)
    data_description = problem.get('dataset_description', {})
    ds = DataDescription(llm)

    if data_description:
        data_path = problem['dataset_path'][:10]
        variable_description = problem['variable_description']
        data_summary = ds.summary(data_description=str(data_description) + '\n' + str(variable_description))
        data_summary = f'Dataset Path:\n{data_path}\n\nData Description:\n{data_summary}'
    else:
        data_summary = ''

    problem['data_summary'] = data_summary
    problem['data_description'] = data_description

    if problem.get('addendum', ''):
        addendum = f"Addendum: \n{problem['addendum']}"
    else:
        addendum = ''
    addendum = addendum[:300]

    problem_str = PROBLEM_PROMPT.format(
        problem_background=problem['background'],
        problem_requirement=problem['problem_requirement'],
        addendum=addendum,
        data_summary=data_summary
    ).strip()
    problem['problem_str'] = problem_str
    return problem_str, problem


def try_load_backup(output_dir: Path, task_id: str):
    """å°è¯•åŠ è½½æœ€è¿‘ä¸€æ¬¡å¤‡ä»½"""
    backup_file = output_dir / f"{task_id}.json"
    if not backup_file.exists():
        return None
    logger.info(f"ğŸ§© æ£€æµ‹åˆ°å¤‡ä»½æ–‡ä»¶ï¼Œå°è¯•ä» {backup_file} æ¢å¤ã€‚")

    with open(backup_file, "r", encoding="utf-8") as f:
        return json.load(f)


def backup_on_criteria_med(task_id:str, problem: dict, solution: dict,
                           dependency_dict:dict, output_dir: Path, error=None) -> None:
    """
    åœ¨ä»»åŠ¡å¤„ç†è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®ç»™å®šæ¡ä»¶å°†å½“å‰çŠ¶æ€ï¼ˆåŒ…æ‹¬é—®é¢˜ã€è§£æ³•ã€ä¾èµ–å…³ç³»åŠé”™è¯¯ä¿¡æ¯ï¼‰å¤‡ä»½ä¸º JSON æ–‡ä»¶ã€‚

    è¯¥å‡½æ•°ä¸»è¦ç”¨äºå®¹é”™å’Œè°ƒè¯•ï¼šå½“ä»»åŠ¡æˆåŠŸå®Œæˆæˆ–å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œè‡ªåŠ¨å°†å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯æŒä¹…åŒ–åˆ°æŒ‡å®šç›®å½•ã€‚
    è‹¥ä¼ å…¥äº† errorï¼ˆé Noneï¼‰ï¼Œåˆ™è®°å½•ä¸ºé”™è¯¯çŠ¶æ€å¹¶è¾“å‡ºè­¦å‘Šæ—¥å¿—ï¼›å¦åˆ™è§†ä¸ºæ­£å¸¸å®Œæˆå¹¶è®°å½•ä¿¡æ¯æ—¥å¿—ã€‚

    Args:
        task_id (str): ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨ä½œå¤‡ä»½æ–‡ä»¶åã€‚
        problem (dict): æè¿°å½“å‰é—®é¢˜çš„å­—å…¸ã€‚
        solution (dict): å½“å‰ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå­—å…¸ã€‚
        dependency_dict (dict): ä¾èµ–å…³ç³»å­—å…¸ã€‚
        output_dir (Path): å¤‡ä»½æ–‡ä»¶ä¿å­˜çš„ç›®å½•è·¯å¾„ã€‚
        error (Exception): è‹¥ä»»åŠ¡å‡ºé”™ï¼Œä¼ å…¥å¼‚å¸¸å¯¹è±¡ï¼›è‹¥ä¸º None è¡¨ç¤ºæ— é”™è¯¯ã€‚

    Results:
        - åœ¨ output_dir ä¸‹åˆ›å»ºä»¥ task_id å‘½åçš„ .json å¤‡ä»½æ–‡ä»¶ã€‚
        - è®°å½• INFO æˆ– WARNING çº§åˆ«æ—¥å¿—ã€‚

    """
    backup_path = output_dir / f"{task_id}.json"
    backup_path.parent.mkdir(parents=True, exist_ok=True)
    backup_data = {
        "task_id": task_id,
        "problem": problem,
        "solution": solution,
        "dependency_dict": dependency_dict,
        "error": str(error),
        "traceback": traceback.format_exc()
    }

    with open(backup_path, "w", encoding="utf-8") as f:
        json.dump(backup_data, f, ensure_ascii=False, indent=2)
    if error:
        logger.warning(f"âš ï¸ å‡ºé”™ï¼Œå·²è‡ªåŠ¨å¤‡ä»½åˆ°ï¼š{backup_path}")
    else:
        logger.info(f"ğŸ§© ä»»åŠ¡å®Œæˆï¼Œå·²è‡ªåŠ¨å¤‡ä»½åˆ°ï¼š{backup_path}")

def log_retry(retry_state: RetryCallState):
    logger.warning(f"ğŸ” {retry_state.fn.__name__} ç¬¬ {retry_state.attempt_number} æ¬¡å°è¯•å¤±è´¥ï¼š{retry_state.outcome.exception()}")

def robust_retry(func):
    return retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, min=2, max=20),
        retry=retry_if_exception_type(Exception),
        reraise=True,
        after=log_retry
    )(func)

# åœ¨æ¯ä¸ªé˜¶æ®µä¸Šå•ç‹¬åº”ç”¨
@robust_retry
def run_problem_understanding(llm, problem_str):
    pu = ProblemUnderstanding(llm)
    return pu.analysis(problem_str, round=0)

@robust_retry
def run_modeling(llm, problem_str, problem_analysis):
    pu = ProblemUnderstanding(llm)
    return pu.modeling(problem_str, problem_analysis, round=0)

@robust_retry
def run_decomposition(llm, problem_str, problem_analysis, modeling_solution, problem_type, config) -> list[str]:
    pd = ProblemDecompose(llm)
    return pd.decompose(problem_str, problem_analysis, modeling_solution, problem_type, config['tasknum'])

@robust_retry
def run_dependency_analysis(llm: LLM, problem_str, problem_analysis, modeling_solution, task_descriptions, with_code) -> dict[str, list[int]]:
    coordinator = Coordinator(llm)
    task_dependency_analysis = coordinator.analyze(
            len(task_descriptions),
            problem_str,
            problem_analysis,
            modeling_solution,
            task_descriptions,
            with_code
        )
    dependency_DAG_string = coordinator.dag_construction(
        len(task_descriptions),
        problem_str,
        problem_analysis,
        modeling_solution,
        task_descriptions,
        task_dependency_analysis
    )
    dependency_DAG = clean_json_txt(dependency_DAG_string)
    if not isinstance(dependency_DAG, dict):
        raise ValueError(
            f"ä¾èµ–åˆ†æè¿”å›ç±»å‹é”™è¯¯ï¼ŒæœŸæœ›dictä½†å¾—åˆ° {type(dependency_DAG)}ï¼ŒåŸå§‹å†…å®¹ä¸ºï¼š{str(dependency_DAG)}"
        )

    if not dependency_DAG:
        raise ValueError("ä¾èµ–åˆ†æè¿”å›ç©ºå­—å…¸ï¼Œè§¦å‘é‡è¯•ã€‚")

    return dependency_DAG


def problem_analysis(llm: LLM, problem_path: Path, config: dict, tmp_dir:Path) -> tuple[dict, dict, bool, dict]:
    """
    å¯¹ç»™å®šé—®é¢˜è¿›è¡Œå¤šé˜¶æ®µåˆ†æï¼ŒåŒ…æ‹¬é—®é¢˜ç†è§£ã€å»ºæ¨¡ã€ä»»åŠ¡åˆ†è§£å’Œä¾èµ–åˆ†æï¼Œå¹¶æ”¯æŒä»ä¸­æ–­å¤„æ¢å¤æ‰§è¡Œã€‚

    è¯¥å‡½æ•°æŒ‰é¡ºåºæ‰§è¡Œå››ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼Œæ¯æ­¥ç»“æœä¼šç¼“å­˜åˆ°å…¨å±€çŠ¶æ€ï¼ˆGLOBAL_STATEï¼‰å¹¶è‡ªåŠ¨å¤‡ä»½åˆ°ä¸´æ—¶ç›®å½•ï¼Œ
    ä»¥ä¾¿åœ¨å¤±è´¥åæ¢å¤ã€‚è‹¥å­˜åœ¨å·²æœ‰å¤‡ä»½ï¼Œåˆ™ä»æœ€åä¸€ä¸ªå®Œæˆçš„é˜¶æ®µç»§ç»­æ‰§è¡Œã€‚

    Args:
        llm: MMAgent.llm.LLM
        problem_path: é—®é¢˜æ–‡ä»¶è·¯å¾„ï¼Œä»ä¸­åŠ è½½é—®é¢˜å†…å®¹ã€‚
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»»åŠ¡åˆ†è§£ç­‰é˜¶æ®µæ‰€éœ€çš„å‚æ•°ã€‚
        tmp_dir (Path): ä¸´æ—¶ç›®å½•è·¯å¾„ï¼Œç”¨äºè¯»å–æˆ–å†™å…¥æ‰§è¡ŒçŠ¶æ€å¤‡ä»½ã€‚

    Returns:
        problem (dict): è§£æåçš„é—®é¢˜ç»“æ„ã€‚
        dependency_dict (dict): ä»»åŠ¡é—´çš„ä¾èµ–å…³ç³»å­—å…¸ã€‚
        with_code (bool): æ˜¯å¦åŒ…å«ä»£ç æ•°æ®é›†ï¼ˆæ ¹æ® problem['dataset_path'] åˆ¤æ–­ï¼‰ã€‚
        solution (dict): å„é˜¶æ®µç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆä¸­é—´ç»“æœã€‚
    """
    global GLOBAL_STATE
    problem_str, problem = get_problem(problem_path, llm)
    GLOBAL_STATE["problem"] = problem

    task_id = find_task_id_from_path(problem_path)
    if not task_id:
        raise ValueError(f"æ— æ³•ä» {problem_path} ä¸­è§£æå‡º task_idã€‚")
    problem_type = task_id.split('_')[-1]

    # å°è¯•æ¢å¤
    solution = GLOBAL_STATE.get("solution", {})
    if not solution:
        solution = {
            'problem_background': problem.get('background'),
            'problem_requirement': problem.get('problem_requirement'),
        }
        GLOBAL_STATE["solution"] = solution

    backup_data = try_load_backup(tmp_dir, task_id)
    if backup_data:
        GLOBAL_STATE.update(backup_data)
        solution = GLOBAL_STATE.get('solution', {})
        start_idx = detect_start_index(solution)
        logger.info(f"ğŸ”„ å°†ä»é˜¶æ®µ {start_idx} æ¢å¤/å¼€å§‹æ‰§è¡Œã€‚")
    else:
        start_idx = 0

    # === Step 1: Problem Understanding ===
    if start_idx <= 0:
        problem_analysis = run_problem_understanding(llm, problem_str)
        solution['problem_analysis'] = problem_analysis
        GLOBAL_STATE["solution"]["problem_analysis"] = problem_analysis
        logger.info('1ï¸âƒ£  Step 1 finished.')

    # === Step 2: Modeling ===
    if start_idx <= 1:
        modeling_solution = run_modeling(llm, problem_str, problem_analysis)
        solution['modeling_solution'] = modeling_solution
        GLOBAL_STATE["solution"]["modeling_solution"] = modeling_solution
        logger.info('2ï¸âƒ£  Step 2 finished.')

    # === Step 3: Decomposition ===
    if start_idx <= 2:
        task_descriptions = run_decomposition(llm, problem_str, problem_analysis, modeling_solution, problem_type, config)
        solution['task_descriptions'] = task_descriptions
        GLOBAL_STATE["solution"]['task_descriptions'] = task_descriptions
        logger.info('3ï¸âƒ£  Step 3 finished.')

    # === Step 4: Dependency Analysis ===
    with_code = len(problem['dataset_path']) > 0
    if start_idx <= 3:
        dependency_dict = run_dependency_analysis(
            llm,
            problem_str,
            problem_analysis,
            modeling_solution,
            task_descriptions,
            with_code
        )
        GLOBAL_STATE["dependency_dict"] = dependency_dict
        logger.info('4ï¸âƒ£  Step 4 finished.')
    else:
        dependency_dict = GLOBAL_STATE.get('dependency_dict', {})

    return problem, dependency_dict, with_code, solution

def detect_start_index(solution: dict) -> int:
    """
    æ ¹æ® solution å†…å®¹å†³å®šä»å“ªä¸ªé˜¶æ®µå¼€å§‹æ¢å¤
    è¿”å›é˜¶æ®µç´¢å¼•ï¼š
      0 - problem_understanding
      1 - modeling
      2 - decomposition
      3 - dependency_analysis
      4 - finished
    """
    if not solution:
        return 0
    if 'problem_analysis' not in solution:
        return 0
    if 'modeling_solution' not in solution:
        return 1
    if 'task_descriptions' not in solution:
        return 2
    # dependency_dict å­˜åœ¨äº GLOBAL_STATEï¼Œè€Œä¸æ€»åœ¨ solution ä¸­
    if GLOBAL_STATE.get('dependency_dict') is None:
        return 3
    return 4

@robust_retry
def generate_criterial(llm, problem: dict, task_descriptions: list[str], subtask_id: int, dependency: list[int], template:str, system: str = 'You are a helpful assistant.'):
    data = {
        'background': problem['background'],
        'question': problem['problem_requirement'],
        'subtask': task_descriptions[subtask_id - 1],
    }
    if dependency:
        dep = [task_descriptions[i-1] for i in dependency]
        previous_subtasks = '\n'.join(dep)
    else:
        previous_subtasks = None
    data['previous_subtasks'] = previous_subtasks

    prompt = populate_template(template, data)
    content = llm.generate(prompt, system, timeout=300)
    return clean_json_txt(content)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--problems", type=Path, nargs="+", default=None)
    parser.add_argument("--problem-dir", type=Path, default="MMBench/CPMCM/problem1")
    parser.add_argument("--criterial-prompt", type=Path, default="eval/prompts/criterial_generate.yaml")
    parser.add_argument("--config-path", type=Path, default="config.yaml")
    parser.add_argument("--output-dir", type=Path, default="MMBench/CPMCM/criteria")
    parser.add_argument("--tmp-dir", type=Path, default="tmp/criteria")
    parser.add_argument('--model-name', type=str, default='Qwen2.5-32B-Instruct')
    args = parser.parse_args()

    problem_paths = []
    if args.problems:
        problem_paths = args.problems
    else:
        for f in args.problem_dir.iterdir():
            problem_paths.append(f)

    config = load_yaml(args.config_path)
    output_dir: Path = args.output_dir
    tmp_dir = args.tmp_dir

    llm = LLM()
    criterial_prompt = load_yaml(args.criterial_prompt)
    user_prompt = criterial_prompt['math_modeling_criteria_generator']['zh']['eval_dimension']
    system_prompt = criterial_prompt['math_modeling_criteria_generator']['system']

    for problem_path in problem_paths:
        task_id = find_task_id_from_path(problem_path)
        if not task_id:
            raise ValueError(f"æ— æ³•ä» {problem_path} ä¸­è§£æå‡º task_id")
        GLOBAL_STATE = {
            "task_id": task_id,
            "problem": None,
            "solution": {},
            "dependency_dict": None
        }
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†é—®é¢˜ï¼š{problem_path}")

        try:
            problem, dependency_dict, with_code, solution = problem_analysis(llm, problem_path, config, tmp_dir)
            backup_on_criteria_med(
                task_id,
                problem,
                solution,
                dependency_dict,
                tmp_dir,
                None,
            )
            assert len(dependency_dict) == len(solution['task_descriptions'])
            dependency_dict = sorted(dependency_dict.items(), key=lambda x: int(x[0]))

            criterial_path = output_dir / f'{task_id}.json'
            criterial_dict = {
                "task_id": GLOBAL_STATE.get("task_id"),
                "problem": problem['problem_requirement'],
                "subtask": []
            }

            if criterial_path.exists():
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶ {criterial_path}ï¼Œå°è¯•æ–­ç‚¹æ¢å¤ã€‚")
                with open(criterial_path, "r", encoding="utf-8") as f:
                    existing = json.load(f)
                done_ids = {item['subtask_id'] for item in existing.get('subtask', [])}
                criterial_dict = existing
            else:
                done_ids = set()

            for id, dependencies in dependency_dict:
                id_int = int(id)
                if id_int in done_ids:
                    logger.info(f"â© è·³è¿‡å·²å®Œæˆä»»åŠ¡ {id_int}")
                    continue

                dependencies = [int(dep) if not isinstance(dep, int) else dep for dep in dependencies]
                criterial = generate_criterial(
                    llm,
                    problem,
                    task_descriptions=solution['task_descriptions'],
                    subtask_id=id_int,
                    dependency=dependencies,
                    template=user_prompt,
                    system=system_prompt,
                )
                logger.info(f"ğŸ” å­ä»»åŠ¡ {id} å¤„ç†å®Œæˆã€‚")
                criterial_dict['subtask'].append({
                    "subtask_id": id_int,
                    "subtask": solution['task_descriptions'][id_int-1],
                    "criteria": criterial
                })
                write_json(criterial_dict, output_dir/f'{task_id}.json', )
            logger.info("ğŸ” å…¨æµç¨‹è¿è¡Œå®Œæˆã€‚")

        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œå¤±è´¥ï¼š{e}")
            backup_on_criteria_med(
                task_id,
                GLOBAL_STATE["problem"],
                GLOBAL_STATE["solution"],
                GLOBAL_STATE["dependency_dict"],
                tmp_dir,
                e
            )
            logger.error("ç¨‹åºå·²å®‰å…¨å¤‡ä»½åé€€å‡ºã€‚")
            exit(1)
