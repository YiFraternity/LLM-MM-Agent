import os
import re
from pathlib import Path
import json
from typing import Dict, Any, List, Union
import logging
import time

from jinja2 import Template, StrictUndefined
import yaml
from openai import OpenAI

from utils.retry_utils import (
    retry_on_api_error,
)


os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

DIMENSION_MAPPING = {
    'é—®é¢˜è¯†åˆ«': 'problem_identify',
    'é—®é¢˜å¤è¿°': 'problem_formulation',
    'å‡è®¾å»ºç«‹': 'assumption_develop',
    'æ¨¡å‹æ„å»º': 'model_construction',
    'æ¨¡å‹æ±‚è§£': 'model_solving',
    'ä»£ç å®ç°': 'code_implementation',
    'ç»“æœåˆ†æ': 'result_analysis',
}

def load_tex_content(latex_file: Union[str, Path]) -> str:
    """
    Load LaTeX file content.
    """
    with open(latex_file, 'r', encoding='utf-8') as file:
        return file.read()

def load_json(file_path: Union[str, Path]):
    """
    Load JSON file content.
    """
    if isinstance(file_path, str):
        file_path = Path(file_path)
    if not file_path.exists():
        return {}
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def load_yaml(config_file: Union[str, Path]) -> Dict[str, Any]:
    """
    Load YAML configuration file.
    """
    with open(config_file, 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)

def load_subtasks_from_criteria(criteria_file: str) -> tuple[list, dict]:
    """
    Load subtasks from criteria JSON file

    Args:
        criteria_file: Path to the criteria JSON file

    Returns:
        tuple: (subtasks_list, subtask_mapping)
    """
    try:
        criteria = load_json(criteria_file)
        # Extract subtask keys that start with 'subtask_'
        subtask_keys = [key for key in criteria.keys() if key.startswith('subtask_')]
        subtask_keys.sort()  # Ensure consistent ordering

        # Create subtasks list and mapping
        subtasks = [f'å­ä»»åŠ¡{int(key.split("_")[1])}' for key in subtask_keys]
        subtask_ens = {f'å­ä»»åŠ¡{int(key.split("_")[1])}': key for key in subtask_keys}

        print(f"Loaded {len(subtasks)} subtasks from criteria file")
        return subtasks, subtask_ens

    except Exception as e:
        print(f"Error loading subtasks from criteria file: {e}")
        raise

def batch_standardize_json(texts: List[str], llm) -> List[Dict]:
    """
    æ‰¹é‡æ ‡å‡†åŒ–JSONæ–‡æœ¬

    å‚æ•°:
        texts: éœ€è¦æ ‡å‡†åŒ–çš„JSONæ–‡æœ¬åˆ—è¡¨
        llm: ç”¨äºæ ‡å‡†åŒ–çš„è¯­è¨€æ¨¡å‹å®ä¾‹

    è¿”å›:
        æ ‡å‡†åŒ–åçš„JSONå¯¹è±¡åˆ—è¡¨
    """
    if not texts:
        return []

    try:
        current_dir = Path(__file__).parent
        prompt_path = current_dir / 'json_standardization.yaml'

        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_template = yaml.safe_load(f).get('json_standardization', '')

        if not prompt_template:
            return [{}] * len(texts)

        # ä¸ºæ¯ä¸ªæ–‡æœ¬å‡†å¤‡æç¤º
        template = Template(prompt_template)
        prompts = [template.render(input_text=text) for text in texts]

        # æ‰¹é‡ç”Ÿæˆå“åº”
        responses = llm.generate(prompts)

        # å¤„ç†å“åº”
        results = []
        for response in responses:
            try:
                standardized_json = response.outputs[0].text.strip()
                results.append(json.loads(standardized_json))
            except (json.JSONDecodeError, IndexError, AttributeError):
                results.append({})

        return results

    except Exception as e:
        print(f"Error during batch JSON standardization: {str(e)}")
        return [{}] * len(texts)

def dict_to_latex_table(data: List[Dict[str, Any]]) -> str:
    """Converts the list of dictionaries into a simple LaTeX tabular environment."""
    if not data:
        return ""

    headers = list(data[0].keys())
    latex_str = "\\begin{tabular}{|" + "|".join(["l"] * len(headers)) + "|}\n"
    latex_str += "\\hline\n"
    latex_str += " & ".join([h.replace("_", "\\_") for h in headers]) + " \\\\\n"
    latex_str += "\\hline\n"
    for row in data:
        escaped_row = [str(v).replace("\\", "\\textbackslash ").replace("_", "\\_").replace("&", "\\&") for v in row.values()]
        latex_str += " & ".join(escaped_row) + " \\\\\n"
    latex_str += "\\hline\n"
    latex_str += "\\end{tabular}\n"

    return latex_str

def clean_json_txt(json_txt: str, standardize: bool = False) -> Union[str, Dict[str, Any], List[Dict[str, Any]]]:
    """
    ä»å¯èƒ½åŒ…å« ```json ... ``` æˆ– ``` ... ``` çš„æ–‡æœ¬ä¸­æå– JSON å¹¶è§£æä¸º dictã€‚

    ä¼˜å…ˆçº§ï¼š
      1) ç¬¬ä¸€ä¸ªæ ‡è®°ä¸º ```json çš„ä»£ç å—ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
      2) ç¬¬ä¸€ä¸ªä»»æ„ ``` ... ``` ä»£ç å—
      3) è§£æåŒ…å«latexå…¬å¼çš„json
      3) æ•´ä¸ªè¾“å…¥å­—ç¬¦ä¸²

    å‚æ•°:
        json_txt: è¦å¤„ç†çš„JSONæ–‡æœ¬
        standardize: æ˜¯å¦å°è¯•æ ‡å‡†åŒ–éæ ‡å‡†JSON

    è¿”å›:
        è§£æåçš„JSONå¯¹è±¡ï¼Œå¦‚æœstandardizeä¸ºTrueä¸”æ ‡å‡†åŒ–å¤±è´¥ï¼Œåˆ™è¿”å›ç©ºå­—å…¸ï¼Œå¦åˆ™è¿”å›json_txt
    """
    if not isinstance(json_txt, str):
        raise TypeError("json_txt must be a str")

    m = re.search(r'```(?:\s*json\b)[\r\n]*([\s\S]*?)```', json_txt, re.IGNORECASE)
    if not m:
        m = re.search(r'```[\r\n]*([\s\S]*?)```', json_txt)

    if m:
        payload = m.group(1).strip()
    else:
        payload = json_txt.strip()

    try:
        return json.loads(payload)
    except json.JSONDecodeError:
        # If it fails, check if the error is likely due to unescaped backslashes
        corrected_payload = re.sub(r'\\', r'\\\\', payload)
        try:
            return json.loads(corrected_payload)
        except json.JSONDecodeError:
            if standardize:
                return {}
            else:
                return json_txt

def populate_template(template: str, variables: dict) -> str:
    """
    Populate a Jinja template with variables.
    """
    compiled_template = Template(template, undefined=StrictUndefined)
    try:
        return compiled_template.render(**variables)
    except Exception as e:
        raise Exception(f"Error during jinja template rendering: {type(e).__name__}: {e}")


def prepare_batch_prompts(prompts_kwargs: List[dict[str, Any]], prompt_template: str, system_prompt='') -> List[List[dict]]:
    """
    Prepare a batch of prompts for inference.
    """
    prompts = [populate_template(prompt_template, prompt_kwarg) for prompt_kwarg in prompts_kwargs]
    if system_prompt == '':
        sys_prompt = 'You are a helpful AI assistant.'
    else:
        sys_prompt = system_prompt
    system_prompts = [sys_prompt for _ in prompts]
    message_list = []
    for prompt, sys_prompt in zip(prompts, system_prompts):
        message_list.append([
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": prompt}
            ])
    return message_list
def write_json(data: Any, file_path: Union[str, Path]) -> None:
    """
    å°†æ•°æ®å†™å…¥ JSON æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ›å»ºçˆ¶ç›®å½•ã€‚

    Args:
        data: è¦å†™å…¥çš„ JSON å¯åºåˆ—åŒ–æ•°æ®ã€‚
        file_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²æˆ– Path å¯¹è±¡ï¼‰ã€‚
    """
    file_path = Path(file_path)
    file_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        with file_path.open('w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except (TypeError, ValueError) as e:
        raise ValueError(f"æ•°æ®æ— æ³•åºåˆ—åŒ–ä¸º JSON: {e}") from e
    except OSError as e:
        raise OSError(f"æ— æ³•å†™å…¥æ–‡ä»¶ {file_path}: {e}") from e

def write_jsonl(data, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'a', encoding='utf-8') as f:
        if isinstance(data, list):
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        else:
            f.write(json.dumps(data, ensure_ascii=False)+ '\n')


def load_jsonl_file(file_path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return [json.loads(line) for line in f]

def load_llm(model_name_or_path, tokenizer_name_or_path=None, gpu_num=1, lora_model_name_or_path=None, max_model_len=32768):
    """
    Load a VLLM model.
    """
    from vllm import LLM, SamplingParams
    kw_args = {
        "model": model_name_or_path,
        "tokenizer": tokenizer_name_or_path,
        "tokenizer_mode": "slow",
        "tensor_parallel_size" : gpu_num,
        "enable_lora": bool(lora_model_name_or_path),
        'max_model_len': max_model_len,
    }
    llm = LLM(**kw_args)
    kwargs={
        "n":1,
        "max_tokens": 4096,
        "top_p":1.0,
        # sampling
        "temperature":0,
        'top_k': 1,
    }
    sampling_params = SamplingParams(**kwargs)
    return llm, sampling_params


def find_task_id_from_path(path: Path) -> Union[None, str]:
    task_id_pattern = re.compile(r'(\d{4}_[A-F])')
    for part in path.parts:
        match = task_id_pattern.search(part)
        if match:
            return match.group(0)

    match = task_id_pattern.search(str(path))
    if match:
        return match.group(0)

    return None

@retry_on_api_error(max_attempts=5, wait_time=3)
def call_openai_api(
    user_prompt: str,
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†ç±»åŠ©æ‰‹ã€‚",
    model: str = "gpt-4-turbo",
    max_tokens: int = 32768,
    temperature: float = 0.3,
    top_p: float = 1.0,
    frequency_penalty: float = 0.0,
    presence_penalty: float = 0.0,
    **kwargs: Any
) -> str:
    """
    Call OpenAI API with retry logic using v1.0+ client.

    Args:
        user_prompt: The prompt to send to the model.
        system_prompt: The system prompt to send to the model.
        model: The model to use (default: gpt-4-turbo).
        max_retries: Maximum number of retry attempts.
        max_tokens: Maximum number of tokens to generate.
        temperature: Sampling temperature.
        top_p: Nucleus sampling threshold.
        frequency_penalty: Frequency penalty.
        presence_penalty: Presence penalty.
        **kwargs: Additional arguments to pass to chat.completions.create.

    Returns:
        The model's response as a string.

    Raises:
        Exception: If all retry attempts fail.
    """
    client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE") or os.getenv("OPENAI_BASE_URL"),
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    # å•æ¬¡è°ƒç”¨é€»è¾‘
    logger.info(f"ğŸš€ Calling OpenAI API | model={model}, temp={temperature}, top_p={top_p}")
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        **kwargs,
    )

    # æ£€æŸ¥è¿”å›å†…å®¹
    content = getattr(response.choices[0].message, "content", None)
    if not content or not content.strip():
        raise ValueError("Empty response from OpenAI API")

    return content.strip()
