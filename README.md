# ğŸ“Š How Far Are We?

## A Problem-Oriented, Stage-Wise Evaluation Framework for Mathematical Modeling

Official implementation for the ACL 2026 submission:

**â€œHow Far Are We? Systematic Evaluation of LLMs vs. Human Experts in Mathematical Contest in Modelingâ€**

---

# ğŸ“– Overview

Large Language Models (LLMs) perform strongly on isolated reasoning benchmarks.
However, real-world problem solving requires **full-process modeling**:

* Understanding the problem
* Formulating a mathematical abstraction
* Constructing and solving models
* Implementing executable solutions
* Validating and analyzing results

Mathematical modeling competitions provide a natural testbed for evaluating this capability.

Unlike traditional benchmarks:

* There is **no unique correct answer**
* Multiple modeling approaches may be valid
* Evaluation relies on expert judgment

This repository implements a **problem-oriented, stage-wise evaluation framework** designed to measure modeling competence faithfully and diagnostically.

---

# â­ Core Contributions

## 1ï¸âƒ£ Problem-Oriented Evaluation

Instead of using generic rubrics, we:

* Decompose each modeling problem into **subtasks**
* Align evaluation criteria with **problem semantics**
* Ground scoring in **task-specific necessary conditions**

This avoids rewarding superficially polished but fundamentally misaligned solutions.

---

## 2ï¸âƒ£ Stage-Wise Evaluation Across the Modeling Pipeline

Each subtask is evaluated along seven canonical modeling stages:

1. Problem Identification
2. Problem Formulation
3. Assumption Development
4. Model Construction
5. Model Solving
6. Code Implementation
7. Result Analysis

This structure enables:

* Fine-grained diagnosis
* Stage-level performance comparison
* Failure pattern tracing

---

## 3ï¸âƒ£ Expert-Aligned Reliability

We validate our framework against independent domain experts.

Using ICC(2,1) to measure agreement:

| Evaluation Scheme | ICC(2,1) vs Expert |
| ----------------- | ------------------ |
| Baseline Rubric   | 0.012              |
| **Ours**          | **0.673**          |



This demonstrates substantially stronger alignment with expert judgment.

---

## 4ï¸âƒ£ Comprehensionâ€“Execution Gap

Applying the framework to state-of-the-art LLMs reveals:

* Strong performance in early stages (problem identification & formulation)
* Significant degradation in:

  * Model solving
  * Code implementation
  * Result validation

Performance declines monotonically across stages.

Scaling model size improves comprehension,
but **does not close the execution gap** .

---

## 5ï¸âƒ£ Failure Pattern Analysis

Stage-wise failure analysis shows that:

* Failures are rarely due to completely wrong ideas.
* Most errors arise from:

  * Missing specification
  * Lack of verification
  * Incomplete derivation
  * Non-reproducible implementation
  * Missing validation

Errors propagate across stages without correction .

---

# ğŸ— Framework Architecture

The evaluation framework operates in two phases:

---

## Phase I: Subtask Decomposition

Each modeling problem is decomposed into:

* Self-contained modeling requirements
* Each requiring a full modeling pipeline

Experts verify that subtasks faithfully represent the original problem intent .

---

## Phase II: Stage-Wise Criteria Instantiation

For each (Subtask Ã— Stage) pair:

* LLM generates fine-grained evaluation criteria
* Experts refine and verify
* Criteria become atomic scoring units

Evaluation is:

* Problem-conditioned
* Stage-aware
* Uniformly applied across models

---

# ğŸ“‚ Dataset

We evaluate on:

**97 problems from the China Postgraduate Mathematical Contest in Modeling (PMCM)**

Characteristics:

* Graduate-level difficulty
* Multi-page real-world tasks
* Multi-method modeling requirements
* Only ~1â€“1.5% gold medal rate

Problems are converted into verified LaTeX format via a structured preprocessing pipeline.

---

# ğŸ“ˆ What This Framework Enables

* Faithful evaluation of open-ended modeling
* Expert-aligned scoring
* Diagnostic stage-wise analysis
* Identification of execution-level weaknesses
* Comparative evaluation across model scales

---

# â–¶ï¸ Usage

### 1. Generate Stage-Wise Criteria

The system generates JSON-based evaluation schemas conditioned on:

* Problem background
* Subtask definition
* Pipeline position

### 2. Stage-Wise Evaluation

Given:

* Subtask
* Modeling report
* Generated criteria

The framework performs:

* Criterion-wise scoring
* Evidence-based justification
* Stage-level aggregation

All outputs are in structured JSON format.

---

# ğŸ“Š Evaluation Protocol

* All models are evaluated under a fixed generation protocol
* Same subtask decomposition applied to all models
* Only the LLM varies

Agreement with experts measured using ICC(2,1) under a two-way random-effects model .

---

# ğŸ”¬ Key Insight

Mathematical modeling competence is not a linear extension of language understanding.

LLMs can:

âœ” Generate plausible modeling ideas

But struggle to:

âœ˜ Execute models rigorously
âœ˜ Provide checkable solutions
âœ˜ Produce reproducible code
âœ˜ Validate results

Improvement requires:

* Process-aware reasoning
* Stage-wise self-correction
* Execution-grounded validation

â€”not just scaling model size .

---

# ğŸ“œ Citation

If you find this work useful, please cite:

```
@article{anonymous2026howfar,
  title={How Far Are We? Systematic Evaluation of LLMs vs. Human Experts in Mathematical Contest in Modeling},
  journal={ACL 2026 Submission}
}
```

---

# ğŸ“Œ Positioning

This repository is:

* âŒ Not a modeling agent
* âŒ Not a solution generator
* âœ… A problem-conditioned evaluation framework
* âœ… A diagnostic tool for modeling competence
* âœ… A reliability-validated scoring system

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å†ç»™ä½ ä¸‰ç§ä¸åŒé£æ ¼ç‰ˆæœ¬ï¼š

1. ğŸ”¥ æ›´â€œACLè®ºæ–‡é£â€ç‰ˆæœ¬ï¼ˆæ›´å…‹åˆ¶ã€æ›´æ­£å¼ï¼‰
2. ğŸš€ æ›´â€œGitHubå¸å¼•åŠ›â€ç‰ˆæœ¬ï¼ˆæ›´ç›´è§‚ã€æ›´æœ‰å†²å‡»åŠ›ï¼‰
3. ğŸ§  æ›´â€œEvaluation Benchmarkâ€å®šä½ç‰ˆæœ¬ï¼ˆå¼ºè°ƒå¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼‰

ä½ æ›´æƒ³è®©è¿™ä¸ª repo çœ‹èµ·æ¥åƒï¼š

* å­¦æœ¯ä»£ç ä»“åº“
* Benchmark é¡¹ç›®
* LLM è¯„æµ‹å·¥å…·
* ç ”ç©¶å‹è¯„ä¼°æ¡†æ¶

å‘Šè¯‰æˆ‘å®šä½ï¼Œæˆ‘ç»™ä½ ä¼˜åŒ–åˆ°æœ€å¼ºç‰ˆæœ¬ã€‚
