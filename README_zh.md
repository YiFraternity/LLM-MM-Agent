# 📊 我们离专家还有多远？

## —— 面向数学建模竞赛的阶段化评估框架

本仓库为 ACL 2026 投稿论文的官方实现：

> **How Far Are We? Systematic Evaluation of LLMs vs. Human Experts in Mathematical Contest in Modeling**

---

# 📖 项目简介

近年来，大语言模型（LLMs）在各类推理基准上表现突出。
但真实世界问题求解并非孤立能力的叠加，而是一个**完整流程的系统性建模过程**：

* 理解问题
* 形式化建模
* 构建数学模型
* 求解与实现
* 验证与分析

数学建模竞赛提供了一个极具挑战性的测试环境：

* 无唯一标准答案
* 多种合理建模路径
* 依赖专家评审而非自动判题

本项目提出一个：

> **问题导向（Problem-Oriented）、阶段化（Stage-Wise）的数学建模评估框架**

用于系统评估 LLM 与人类专家之间的建模能力差距。

---

# ⭐ 核心贡献

## 1️⃣ 问题语义驱动的评估方式

传统评估方式通常采用：

* 固定的、问题无关的评分维度
* 粗粒度的报告层面打分

我们的方法：

* 将每道建模题目分解为若干**子任务（Subtasks）**
* 对每个子任务构建**问题语义条件化的评分标准**
* 用“必要条件”约束评分，而非表面完整性

避免了“报告写得好但问题没解决”的假高分现象。

---

## 2️⃣ 覆盖完整建模流程的七阶段评估

每个子任务均按照七个标准建模阶段进行评估：

1. 问题识别
2. 问题复述
3. 假设建立
4. 模型构建
5. 模型求解
6. 代码实现
7. 结果分析

这种结构带来：

* 细粒度能力诊断
* 阶段性性能对比
* 失败路径追踪
* 错误传播分析

---

## 3️⃣ 与专家判断高度对齐

我们邀请获奖建模选手进行独立评分，并计算 ICC(2,1) 一致性指标。

| 评估方式    | 与专家一致性 ICC(2,1) |
| ------- | --------------- |
| 传统基线评分  | 0.012           |
| **本框架** | **0.673**       |

结果表明：

> 问题导向 + 阶段化评估显著提升与专家判断的一致性。

---

## 4️⃣ 揭示“理解—执行断层”

使用本框架评估多种主流 LLM 后发现：

* 在“问题识别、问题建模”阶段接近专家水平
* 在“模型求解、代码实现、结果验证”阶段显著退化
* 分数沿建模流程呈单调下降趋势

更重要的是：

> 模型规模扩大并不能有效弥补执行阶段的能力缺陷。

我们将这一现象定义为：

# 🧠 理解—执行断层（Comprehension–Execution Gap）

---

## 5️⃣ 失败模式归因分析

阶段化分析表明：

LLM 很少犯“完全错误”的建模思想问题。

主要失败原因包括：

* 假设未验证
* 模型推导不完整
* 求解过程缺失关键步骤
* 代码不可执行或不可复现
* 结果缺乏验证与敏感性分析

错误往往：

> 在早期阶段产生
> 在后续阶段传播
> 但从未被回溯修正

---

# 🏗 框架结构

评估框架包含两个核心阶段：

---

## 第一阶段：子任务分解

每道建模题目被分解为：

* 独立的建模子任务
* 每个子任务必须可单独完成完整建模流程

专家参与验证分解是否覆盖原题核心要求。

---

## 第二阶段：阶段化评分细则构建

对于每个：

> 子任务 × 建模阶段

生成细粒度评分标准：

* 明确任务目标
* 明确必要条件
* 明确可检验指标
* 明确评分区间

所有评分标准：

* 在评估前固定
* 对所有模型统一使用
* 保证公平与可复现

---

# 📂 数据集

本研究使用：

> 中国研究生数学建模竞赛（PMCM）97 道题目

特点：

* 研究生级难度
* 多页真实问题场景
* 综合多种建模方法
* 金奖率仅约 1–1.5%

题目经过：

* PDF → LaTeX 结构化转换
* 人工校对验证
* 元数据标注

保证语义与数学表达完整性。

---

# 📈 本框架的意义

该评估体系可以：

* 精准评估开放式建模能力
* 诊断 LLM 在建模流程中的薄弱环节
* 比较不同规模模型的阶段性能
* 分析错误传播路径
* 为“如何提升执行能力”提供证据基础

---

# 🔬 核心发现

数学建模能力不是语言理解能力的线性延伸。

当前 LLM：

✔ 能提出合理建模思路
✘ 难以严格执行
✘ 难以给出可检验解
✘ 难以实现可复现代码
✘ 难以完成严格验证

未来改进方向应聚焦：

* 过程感知推理（Process-Aware Reasoning）
* 阶段化自校正机制（Stage-wise Self-Correction）
* 可执行性与可验证性增强

而不仅仅是模型规模扩展。

---

# ▶️ 使用方式

1. 输入建模题目
2. 生成子任务分解
3. 构建阶段化评分标准（JSON 格式）
4. 对模型生成的建模报告进行逐项评分
5. 输出结构化阶段评分结果

所有评估结果均为：

* 可复现
* 可追踪
* 可解释

---

# 📌 项目定位

本项目是：

* ❌ 不是建模解题系统
* ❌ 不是 Agent 框架
* ✅ 是问题导向的评估体系
* ✅ 是阶段化诊断工具
* ✅ 是专家对齐的评分系统

---

# 📜 引用方式

如使用本框架，请引用：

```
@article{anonymous2026howfar,
  title={How Far Are We? Systematic Evaluation of LLMs vs. Human Experts in Mathematical Contest in Modeling},
  journal={ACL 2026 Submission}
}
```